{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SK2bwmq6mgJs",
        "hSz3a0FCmanw",
        "xHhXrV6IT81H",
        "8sh_mibwaWs2",
        "UlrUzQWeUPVr",
        "tiLuOhVWqkQu",
        "YYgz81gviK0v",
        "wsxGlIDpnlaF",
        "sRMnL-4NtYfG",
        "2i9LzlE1bSD5",
        "_I2mGTF-ngyv",
        "sJeJ0UbaoC_w",
        "CMcaFeeJo_wu",
        "7MXOW8HlpW9P",
        "B1LcuQiut9lf",
        "6TvOoTC0vgR3",
        "iVSMUThEwAV2",
        "But8_tPFwqAf",
        "KqFZ0tGABW94",
        "7JLMsbTbk1D2",
        "t4LdNj7Xxxm3",
        "31ntKCQFrV_l",
        "OKnsGTW77xmn",
        "g0PqDNp7w_dK",
        "cGXot-1V2UuA",
        "9-ZZZYQO7Ou9",
        "AVNcSTvH8wjP",
        "mbEz5cMcWWEh",
        "zjszFUYbYsJ4",
        "IE0fC_lBcFb8",
        "-DghMUWcisth",
        "aG-uPmX5kEV-",
        "GzI6L19WkBSz",
        "ECtP6TncmPvL",
        "57TjBvK2mr6V",
        "la-VvQ0km3Mn",
        "bjBB1IqknCLs",
        "3oMJ0P6_nfIX",
        "mGsf4YGanlpB",
        "cqWwF9XZo0n2",
        "Kx9tOscaCZBS",
        "Y1RAlyZZKPQg"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## ðŸ“– Introduction\n"
      ],
      "metadata": {
        "id": "SK2bwmq6mgJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every time a customer makes a purchase â€” whether in-store or online â€” they leave behind a trail of information :\n",
        "* the date of the purchase,\n",
        "* the store location,\n",
        "* the product chosen,\n",
        "* the quantity bought,\n",
        "* the amount spent.\n",
        "\n",
        "On the surface, these records may appear as nothing more than rows in a spreadsheet :\n",
        "* numbers,\n",
        "* dates,\n",
        "* product codes.\n",
        "\n",
        "But transactions alone donâ€™t tell the full story. Behind every purchase is a customer, each with their own lifestyle, preferences, and spending habits. By connecting transaction records with customer profiles â€” such as their lifestage and spending segment â€” we gain a richer perspective. We begin to see how different customer groups shop, which products appeal to specific demographics, and how spending behavior changes across life stages.\n",
        "\n",
        "When visualized, these datasets reveal powerful patterns. We can observe which products surge in popularity during certain seasons, how budget-conscious customers differ from premium ones, and which promotions resonate with specific segments. What once looked like scattered data points now becomes a narrative about people, choices, and retail dynamics.\n",
        "\n",
        "Thatâ€™s the true value of combining transaction and customer data :\n",
        "\n",
        "* It turns raw numbers into meaningful insights. By analyzing these patterns, businesses can optimize inventory, tailor promotions, and better understand their customers â€” ultimately making smarter, data-driven decisions in a competitive retail landscape.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Ggf9vsruaaq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "##ðŸ“ Before We Dive In..."
      ],
      "metadata": {
        "id": "hSz3a0FCmanw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start digging deep into the dataset, letâ€™s pause for a moment and discuss a couple of questions for better understanding of our intent â€” why we are analyzing this data, what kind of insights we hope to uncover, and how our choice of tools will help us achieve these goals.   \n",
        "\n",
        "**Q1 : What do we understand by Time-Series Data?**  \n",
        "In our project, time-series data simply means data that is recorded in order over time. For example, every transaction in the transaction dataset has a date attached to it. Once we add features like week, month, or quarter, we can start looking at how sales change across different periods.\n",
        "\n",
        "This allows us to see trends such as :\n",
        "\n",
        "* Do sales increase during festive seasons?\n",
        "\n",
        "* Are weekends busier than weekdays?\n",
        "\n",
        "* How do customer buying habits shift across months or years?\n",
        "\n",
        "In short, time-series data doesnâ€™t just tell us what was sold, it helps us understand when sales happen and how patterns evolve over time.\n",
        "\n",
        "**Q2 : Was choosing Python a good decision for this analysis?**  \n",
        "Yes, Python is definitely a smart choice for this kind of analysis. It offers :\n",
        "\n",
        "* **Easy data handling** with pandas, which helps us clean, merge, and work with large datasets like ours (over 260,000 transactions).\n",
        "\n",
        "* **Strong support** for time-series analysis with libraries like pandas and statsmodels.\n",
        "\n",
        "* **Clear visualizations** using matplotlib, seaborn, or plotly to spot trends and patterns.\n",
        "\n",
        "* **Scalability** to work with large amounts of data without much trouble.\n",
        "\n",
        "* **Flexibility** to go beyond simple analysis into predictive modeling or machine learning if needed.\n",
        "\n",
        "\n",
        "So, for a retail analytics project like this â€” where we want to connect customer information with transaction data over time â€” Python is a very good option.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4cn0O9BNUCZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "##ðŸŽ¯ Objective :\n"
      ],
      "metadata": {
        "id": "xHhXrV6IT81H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this project is to perform **Exploratory Data Analysis (EDA)** on the `Retail Sales` dataset to uncover meaningful insights regarding customer purchasing behavior and transaction patterns. The dataset contains detailed information on customer demographics, product categories, purchase dates, and transaction values. Through systematic exploration, this project aims to illustrate the underlying trends, relationships, and anomalies within the data.\n",
        "\n",
        "The analysis includes identifying customer segments, understanding product-level performance, and exploring temporal patterns in purchase behavior. Furthermore, the study seeks to determine whether specific factors such as age, location, or product category significantly influence spending patterns. To achieve these goals, the study is guided by the following specific purposes:\n",
        "\n",
        "- **Identifying store-wise and product-level sales trends**\n",
        "- **Analyzing customer purchase behavior based on demographics**\n",
        "- **Examining temporal patterns in transactions**\n",
        "- **Exploring the relationship between price, quantity purchased, and total sales**  \n",
        "- **Studying brand-level performance and customer preferences**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Kxws1ifIdEO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## âš™ï¸ Setting Things Up\n"
      ],
      "metadata": {
        "id": "8sh_mibwaWs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we begin our analysis, we need to make sure we have the right tools.\n",
        "\n",
        "In the next coding cell, weâ€™ll import the required Python libraries that will help us clean, analyze, and visualize the Retail transaction and customer datasets."
      ],
      "metadata": {
        "id": "4lQbba0f8Q6F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "GCdoeFdjmw-I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import re\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Settings for plots\n",
        "sns.set(rc={\"figure.figsize\":(10,6)})"
      ],
      "metadata": {
        "id": "hpPucjq5xkAL"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## ðŸ“‚ Loading Our Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "UlrUzQWeUPVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now that our setup is ready, itâ€™s time to bring in the data weâ€™ll be working with. In the next coding cell, weâ€™ll load the Retail transaction and customer datasets from our files.\n",
        "\n",
        "Once the data is loaded, weâ€™ll display the first few rows to get a quick overview of what each dataset contains before moving into cleaning, analysis, and visualization."
      ],
      "metadata": {
        "id": "sHsIGF1K8XTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file1_id = \"1m60-1Tj3at_tKAG-TGzf68yrxTTyHQfC\"\n",
        "file2_id = \"1QvOOKGg1cj-Rhh4gSKa_DNAk6IHMgAuM\"\n",
        "\n",
        "url1 = f\"https://drive.google.com/uc?export=download&id={file1_id}\"\n",
        "url2 = f\"https://drive.google.com/uc?export=download&id={file2_id}\"\n",
        "\n",
        "try:\n",
        "  # Read Excel and CSV directly into pandas\n",
        "  transactionData = pd.read_excel(url1)\n",
        "  customerData = pd.read_csv(url2)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")"
      ],
      "metadata": {
        "id": "raciHeIiUjvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##1. Data Overview :"
      ],
      "metadata": {
        "id": "tiLuOhVWqkQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1. General overview :"
      ],
      "metadata": {
        "id": "YYgz81gviK0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step in any analysis is to first understand the data. Let's take a look at each of the datasets provided."
      ],
      "metadata": {
        "id": "ev_y1D5mcWGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the data for further observations :\n",
        "print(\" The transaction data : \")\n",
        "display(transactionData)\n",
        "print(\"\\n The customer data : \")\n",
        "display(customerData)"
      ],
      "metadata": {
        "id": "0HHyHsTHcL9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hereâ€™s the data overview of the last two uploaded datasets:\n",
        "1. transactionData :\n",
        "\n",
        "This dataset contains 264,836 transaction records of chip sales.\n",
        "\n",
        "Variable Descriptions:\n",
        "\n",
        "* `DATE` : Date of transaction (currently stored as Excel serial date, needs conversion to actual date).\n",
        "\n",
        "* `STORE_NBR` : Store number where the purchase occurred.\n",
        "\n",
        "* `LYLTY_CARD_NBR` : Loyalty card number of the purchasing customer.\n",
        "\n",
        "* `TXN_ID` : Unique transaction identifier.\n",
        "\n",
        "* `PROD_NBR` : Unique product identifier.\n",
        "\n",
        "* `PROD_NAME` : Name of the product purchased (e.g., Smiths Crinkle Cut Chips Chicken 170g).\n",
        "\n",
        "* `PROD_QTY` : Quantity of the product purchased in that transaction.\n",
        "\n",
        "* `TOT_SALES` : Total sales value for the transaction (in dollars).\n",
        "\n",
        "2. customerData :\n",
        "\n",
        "This dataset contains 72,637 customer records describing customer profiles.\n",
        "\n",
        "Variable Descriptions :\n",
        "\n",
        "* `LYLTY_CARD_NBR` : Unique loyalty card number identifying each customer.\n",
        "\n",
        "* `LIFESTAGE` : Customerâ€™s life stage group (e.g., YOUNG SINGLES/COUPLES, MIDAGE SINGLES/COUPLES, OLDER FAMILIES).\n",
        "\n",
        "* `PREMIUM_CUSTOMER` : Customer segmentation based on spending habits (e.g., Premium, Mainstream, Budget).\n"
      ],
      "metadata": {
        "id": "lGV711cyW5gx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2. Checking format of the transaction data :"
      ],
      "metadata": {
        "id": "wsxGlIDpnlaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use `info()` to look at the format of each column and see a sample of the data. As we have read in the dataset as a `pd` object, we can also run `transactionData` in the console to see a sample of the data or use `transactionData.head()` to look at the first 5 rows."
      ],
      "metadata": {
        "id": "iTVflsnQc_uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactionData.info()"
      ],
      "metadata": {
        "id": "bMc5YdUndES8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 264,836 records (rows) and 8 features (columns).\n",
        "\n",
        "Out of the 8 features:\n",
        "\n",
        "* 1 feature has floating-point (decimal) data type.\n",
        "\n",
        "* 6 features have integer (whole number) data type.\n",
        "\n",
        "* 1 feature has object (string) data type.\n",
        "\n",
        "The `DATE` column is stored as an integer (Excel serial date) and will need conversion into a proper datetime format for analysis.\n",
        "\n",
        "The dataset has no null values (all Non-Null Counts match the number of rows).\n",
        "\n",
        "The memory requirement for this dataset is approximately 16.2 MB."
      ],
      "metadata": {
        "id": "pO45r4W4dQzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.3. Checking format of the customer data :"
      ],
      "metadata": {
        "id": "sRMnL-4NtYfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The structure of customer data : \\n\")\n",
        "customerData.info()"
      ],
      "metadata": {
        "id": "iMVV11lpyaHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 72,637 records (rows) and 3 features (columns).\n",
        "\n",
        "Out of the 3 features:\n",
        "\n",
        "* 1 feature has integer (whole number) data type.\n",
        "\n",
        "* 2 features have object (string) data type.\n",
        "\n",
        "The dataset has no null values (all Non-Null Counts equal the total number of rows).\n",
        "\n",
        "The memory requirement for this dataset is approximately 1.7 MB.\n",
        "\n"
      ],
      "metadata": {
        "id": "wt1kqlu1bWcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OnEXSutX2VuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. Data Preprocessing :"
      ],
      "metadata": {
        "id": "2i9LzlE1bSD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the overview, the dataset will not need a lot of data cleaning. However, there are certain transformations that needs to be done to ready the data for the analysis. Specifically, the `DATE` column and some others are not necessary for this particular analysis and will be removed. Feature engineering will also be done."
      ],
      "metadata": {
        "id": "KJK-U6_9b5i0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1. Examining transaction data :\n"
      ],
      "metadata": {
        "id": "_I2mGTF-ngyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting DATE column to a date format. A quick search online tells us that CSV and Excel integer dates begin on 30 Dec 1899.**"
      ],
      "metadata": {
        "id": "9X26kWAEdR8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactionData['DATE'] = pd.to_datetime(transactionData['DATE'], unit='D', origin= '1899-12-30')\n",
        "transactionData.head()"
      ],
      "metadata": {
        "id": "xXXawWT_dZZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2.1.1. Finding target products for inference :"
      ],
      "metadata": {
        "id": "sJeJ0UbaoC_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should check that we are looking at the right products by examining `PROD_NAME`."
      ],
      "metadata": {
        "id": "WNYM1SULef0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(transactionData['PROD_NAME'].describe())    # Generate a summary of the PROD_NAME column.\n",
        "unique_items = transactionData['PROD_NAME'].unique()   # Extract the unique items of the transactionData.\n",
        "print(f\"\\nSome unique items of the transactionData :  \\n {unique_items[0:20]}\")"
      ],
      "metadata": {
        "id": "7vtJDuZRegq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like we are definitely looking at potato chips but how can we check that these are all chips?\n",
        "\n",
        "We can do some basic text analysis by summarising the individual words in the product name."
      ],
      "metadata": {
        "id": "cbk-noNoeuTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# It takes the unique_item list (which contains unique product names) and joins all the elements into a single string.\n",
        "# It places a space between each product name.\n",
        "all_products = \" \".join(unique_items)\n",
        "\n",
        "# Replace digits and special characters with space and Keep only letters\n",
        "clean_text = re.sub(r'[^A-Za-z]', ' ', all_products)\n",
        "\n",
        "# Split into individual words and lowercase for uniformity\n",
        "words = clean_text.lower().split()\n",
        "# Count word frequencies\n",
        "\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Let's look at the most common words by counting the number of times a word appears\n",
        "# Sorting them by this frequency in order of highest to lowest frequency\n",
        "\n",
        "# Convert to DataFrame\n",
        "word_freq_df = pd.DataFrame(word_counts.items(), columns=['word', 'frequency'])\n",
        "\n",
        "# Sorting them\n",
        "word_freq_df = word_freq_df.sort_values(by='frequency', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nDistinct words sorted by frequency (after cleaning) : \")\n",
        "display(word_freq_df)"
      ],
      "metadata": {
        "id": "lIr1OBoqezcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are salsa and dip products in the dataset but we are only interested in the chips category, so let's remove these."
      ],
      "metadata": {
        "id": "AYilgoGhgXdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Name of the products that contains salsa or dip.\n",
        "non_chips = transactionData[\"PROD_NAME\"].str.contains('salsa | dip',case = False)\n",
        "\n",
        "# The transactions that contain salsa and dip products.\n",
        "transactionData_non_chips = transactionData[non_chips]\n",
        "print(f\"\\nThe products that does not contain any chips item : \\n\")\n",
        "display(transactionData_non_chips)\n",
        "\n",
        "# The transactions that contain chips products.\n",
        "transactionData_chips = transactionData[~non_chips]\n",
        "print(f\"\\n\\nThe products that contain chips item : \\n\")\n",
        "display(transactionData_chips)\n",
        "\n",
        "print(f\"\\n\\nThe numer of times each product have bought : \\n\")\n",
        "\n",
        "# Number of trensactions for each product\n",
        "transactionData_chips.groupby('PROD_NAME').size().reset_index(name='Transaction_count')"
      ],
      "metadata": {
        "id": "ueJ2deG3gYID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2.1.2. Checking for outliers :"
      ],
      "metadata": {
        "id": "CMcaFeeJo_wu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can use `describe()` to check summary statistics such as mean, min and max values for each feature to see if there are any obvious outliers in the data and if there are any nulls in any of the columns (`NaN : number of nulls` will appear in the output if there are any nulls)."
      ],
      "metadata": {
        "id": "Mlq-17ZehLXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactionData_chips.describe()    # Summarise the data to check for nulls and possible outliers"
      ],
      "metadata": {
        "id": "iwH12AhmhL9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no nulls in the columns but product quantity appears to have an outlier which we should investigate further. Let's investigate the case where 200 packets of chips are bought in one transaction."
      ],
      "metadata": {
        "id": "xd3oAdYHhQGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The transactions that contains 200(maximum) product quantity\n",
        "max_product_quantity = transactionData_chips[transactionData_chips['PROD_QTY'] == 200]\n",
        "print(f\"\\nThe transactions that have max quantity : \\n\")\n",
        "display(max_product_quantity)"
      ],
      "metadata": {
        "id": "k3gWTcFNhUEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two transactions where 200 packets of chips are bought in one transaction and both of these transactions were by the same customer."
      ],
      "metadata": {
        "id": "dyTt690XhZEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The other transactions that customer made.\n",
        "max_product_quantity_customer = transactionData_chips[transactionData_chips['LYLTY_CARD_NBR'] == 226000]\n",
        "print(f\"\\nThe customers who have max quantity transaction : \\n\")\n",
        "display(max_product_quantity_customer)"
      ],
      "metadata": {
        "id": "u6bMtONYhbnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like this customer has only had the two transactions over the year and is not an ordinary retail customer. The customer might be buying chips for commercial purposes instead. We'll remove this loyalty card number from further analysis."
      ],
      "metadata": {
        "id": "IgJ5JFAYheHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out the customer based on the loyalty card number.\n",
        "transactionData_chips_cleaned = transactionData_chips.drop(max_product_quantity_customer.index)\n",
        "\n",
        "# Re-examine transaction data\n",
        "transactionData_chips_cleaned.describe()"
      ],
      "metadata": {
        "id": "HWCSoehzhk9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2.1.3. Handling missing values :"
      ],
      "metadata": {
        "id": "7MXOW8HlpW9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Now, let's look at the number of transaction lines over time to see if there are any obvious data issues such as missing data."
      ],
      "metadata": {
        "id": "1-2yoKYIiFnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping transactions by date, counts how many occurred each day, resets the result into a DataFrame, and sorts it by date in ascending order.\n",
        "transactionData_by_date = transactionData_chips_cleaned.groupby('DATE').size().reset_index(name='transaction_count').sort_values(by='DATE',ascending=True)\n",
        "print(f\"The transactions by date : \")\n",
        "display(transactionData_by_date)"
      ],
      "metadata": {
        "id": "sNhbdUmtih7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's only 364 rows, meaning only 364 dates but from 2018-07-01\tto 2019-06-30\tthere are 365 dates, which indicates a missing date. Let's create a sequence of dates from 1 Jul 2018 to 30 Jun 2019 and use this to create a chart of number of transactions over time to find the missing date."
      ],
      "metadata": {
        "id": "9K4a4U_jipFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_counts = pd.date_range(start = '2018-7-1', end = '30/6/2019', freq = 'D')\n",
        "\n",
        "# Create a column of dates that includes every day from 1 Jul 2018 to 30 Jun 2019\n",
        "daily_counts_df = pd.DataFrame(daily_counts, columns=['DATE'])\n",
        "\n",
        "# Join that column of dates onto the transactionData.\n",
        "transactionData_chips_cleaned_by_day = daily_counts_df.merge(transactionData_chips_cleaned.sort_values(by = 'DATE'), on='DATE', how='left')\n",
        "\n",
        "# Fill the missng value by 0\n",
        "transactionData_chips_cleaned_by_day = transactionData_chips_cleaned_by_day.fillna(0)\n",
        "name = ['STORE_NBR', 'LYLTY_CARD_NBR', 'TXN_ID', 'PROD_NBR','PROD_QTY']\n",
        "transactionData_chips_cleaned_by_day[name] = transactionData_chips_cleaned_by_day[name].astype(int)\n",
        "\n",
        "print(f\"The transaction count with all : \")\n",
        "display(transactionData_chips_cleaned_by_day)\n",
        "\n",
        "# Extracting the date containing 0 as total sales\n",
        "missing_dates = transactionData_chips_cleaned_by_day[transactionData_chips_cleaned_by_day['TOT_SALES'] == 0]['DATE']\n",
        "print(\"\\nMissing date(s) with no transactions: \\n\", missing_dates)"
      ],
      "metadata": {
        "id": "qtyG1Furizec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2.1.4. Inference of missing date :"
      ],
      "metadata": {
        "id": "B1LcuQiut9lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's plot the transactionData by date, for further observation"
      ],
      "metadata": {
        "id": "uray3DT_vNDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactionData_by_day_table = transactionData_chips_cleaned_by_day.groupby('DATE').size().reset_index(name='transaction_count').sort_values(by='DATE',ascending=True)\n",
        "print(\"The amount of daily transactions : \")\n",
        "display(transactionData_by_day_table)\n",
        "\n",
        "print(\"\\nThe Plot of daily transactions over the whole time : \")\n",
        "plt.plot(transactionData_by_day_table['DATE'], transactionData_by_day_table['transaction_count'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Transaction Count')\n",
        "plt.title('Transaction Count Over Time')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KWoRSqDeu_4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there is an increase in purchases in December and a break in late December. Let's zoom in on this."
      ],
      "metadata": {
        "id": "xtRzW6tnvTwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter to December\n",
        "transactions_by_day_zoomed = transactionData_by_day_table[(transactionData_by_day_table['DATE'] >= '2018-12-01') &\n",
        "                                                          (transactionData_by_day_table['DATE'] <= '2018-12-31')]\n",
        "plt.plot(transactions_by_day_zoomed['DATE'], transactions_by_day_zoomed['transaction_count'])\n",
        "plt.xticks(rotation=90)   # display the x_axis labels at an angel of 90 degrees\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Transaction Count')\n",
        "plt.title('Transaction Count Over Time (Zoomed)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dEVdqHZ2vI24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the increase in sales occurs in the lead-up to Christmas and that there are zero sales on Christmas day itself. This is due to shops being closed on Christmas day."
      ],
      "metadata": {
        "id": "onEquqhwvda4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2.1.5. Feature extraction :"
      ],
      "metadata": {
        "id": "6TvOoTC0vgR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we are satisfied that the data no longer has outliers, we can move on to creating other features such as brand of chips or pack size from `PROD_NAME`."
      ],
      "metadata": {
        "id": "ta2vLCAivvyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####2.1.5.1 Checking pack size :"
      ],
      "metadata": {
        "id": "iVSMUThEwAV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start with pack size."
      ],
      "metadata": {
        "id": "XDvgUf9UcmSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the unique items of PROD_NAME from the transactionData.\n",
        "item_names = transactionData_chips_cleaned['PROD_NAME']\n",
        "\n",
        "# It takes the unique_item list (which contains unique product names) and\n",
        "# joins all the elements into a single string. It places a space between each product name.\n",
        "all_products = \"  \".join(item_names )\n",
        "\n",
        "# It extracts all substrings from all_products that consist of one or more digits immediately followed by the letter 'g' (e.g., '175g').\n",
        "PACK_SIZE = re.findall(r'\\d+', all_products)\n"
      ],
      "metadata": {
        "id": "SbqTq3WGv5v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The largest size is 380g and the smallest size is 70g - seems sensible!"
      ],
      "metadata": {
        "id": "ACJYb8JXwS7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####2.1.5.2 Checking brands :"
      ],
      "metadata": {
        "id": "But8_tPFwqAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to create brands, we can use the first word in PROD_NAME to work out the brand name."
      ],
      "metadata": {
        "id": "rHVBOahkw8QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the items of PROD_NAME from the transactionData.\n",
        "unique_items = transactionData_chips_cleaned['PROD_NAME']\n",
        "\n",
        "# Extracts the first word (usually brand name) from each product name in unique_item by cleaning spaces and splitting the text.\n",
        "BRANDS = [''.join(item.strip().split()[0]) for item in unique_items]\n",
        "BRANDS_count_table = pd.DataFrame(Counter(BRANDS).items(),columns=['BRAND_NAME','frequency']).sort_values(by='frequency', ascending=False)\n",
        "print(\"Distinct brands sorted by frequency : \")\n",
        "display(BRANDS_count_table)"
      ],
      "metadata": {
        "id": "-K48UBz-w7S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the brand names look like they are of the same brands - such as RED and RRD, which are both Red Rock Deli chips and WW is Woolworths. Let's replace these."
      ],
      "metadata": {
        "id": "Y8Iy_u8YxNln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replacements = {\n",
        "    'Red': 'Red Rock Deli',\n",
        "    'RRD': 'Red Rock Deli',\n",
        "    'WW': 'Woolworths',\n",
        "    'Smith': 'Smiths'\n",
        "}\n",
        "# Replace brands if in dictionary, otherwise keep same\n",
        "BRANDS = [replacements.get(brand, brand) for brand in BRANDS]\n",
        "\n",
        "BRANDS_count_table = pd.DataFrame(Counter(BRANDS).items(),columns=['BRAND_NAME','frequency']).sort_values(by='frequency', ascending=False)\n",
        "\n",
        "print(\"Distinct brands sorted by frequency after combining similar brands : \")\n",
        "display(BRANDS_count_table)"
      ],
      "metadata": {
        "id": "5RsMy5ucxOdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####2.1.5.3 Checking selling price :"
      ],
      "metadata": {
        "id": "KqFZ0tGABW94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating selling price for the each product\n",
        "SELLING_PRICE = transactionData_chips_cleaned['TOT_SALES'] / transactionData_chips_cleaned['PROD_QTY']"
      ],
      "metadata": {
        "id": "O07C-kKjBi8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####2.1.5.4. Checking year, month, quarter, day and week :"
      ],
      "metadata": {
        "id": "7JLMsbTbk1D2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "extracting specific date values from the `DATE` column for trend of sales with respect to different time units"
      ],
      "metadata": {
        "id": "hya6tnwsq81k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATE = pd.to_datetime(transactionData_chips_cleaned['DATE'])\n",
        "YEAR = DATE.dt.year\n",
        "MONTH = DATE.dt.month\n",
        "QUARTER = DATE.dt.quarter\n",
        "WEEK = ((DATE.dt.day - 1) // 7 + 1)"
      ],
      "metadata": {
        "id": "T-Gkqdv-lTfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2.1.6. Getting cleaned data :"
      ],
      "metadata": {
        "id": "t4LdNj7Xxxm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactionData_chips_cleaned['BRANDS'] = BRANDS\n",
        "transactionData_chips_cleaned['PACK_SIZE'] = PACK_SIZE\n",
        "transactionData_chips_cleaned['SELLING_PRICE'] = SELLING_PRICE\n",
        "transactionData_chips_cleaned['YEAR'] = YEAR\n",
        "transactionData_chips_cleaned['MONTH'] = MONTH\n",
        "transactionData_chips_cleaned['QUARTER'] = QUARTER\n",
        "transactionData_chips_cleaned['WEEK'] = WEEK\n",
        "\n",
        "transactionData_cleaned = transactionData_chips_cleaned[['DATE', 'YEAR', 'MONTH', 'QUARTER', 'WEEK', 'STORE_NBR', 'LYLTY_CARD_NBR', 'TXN_ID', 'PROD_NBR',\n",
        "                                                         'BRANDS', 'PROD_NAME', 'PACK_SIZE', 'PROD_QTY', 'SELLING_PRICE', 'TOT_SALES']]\n",
        "print(f\"The cleaned data : \")\n",
        "display(transactionData_cleaned)"
      ],
      "metadata": {
        "id": "SlOOzBmxx7T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2. Examining customer data :"
      ],
      "metadata": {
        "id": "31ntKCQFrV_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we are happy with the transaction dataset, let's have a look at the customer dataset."
      ],
      "metadata": {
        "id": "rq5kQ_gD0iMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the initial overview of the dataset, no data cleaning or transformation is required, as the dataset is already structured and ready for analysis."
      ],
      "metadata": {
        "id": "c-4Ve3nozuWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.3 The final data :"
      ],
      "metadata": {
        "id": "3kQ9JHX41vZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2.3.1. Obtainig the data :"
      ],
      "metadata": {
        "id": "OKnsGTW77xmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge transaction data to customer data"
      ],
      "metadata": {
        "id": "w6a7-ngP1yqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = transactionData_cleaned.merge(customerData, on = 'LYLTY_CARD_NBR', how = 'left')\n",
        "print(f\"The merged data : \")\n",
        "display(data)"
      ],
      "metadata": {
        "id": "y4IAwIxH2QC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2.3.2. Checking format of the data :"
      ],
      "metadata": {
        "id": "g0PqDNp7w_dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "aLFXh_HOxOlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 246,740 records (rows) and 17 features (columns).\n",
        "\n",
        "Out of the 17 features :\n",
        "\n",
        "- 1 feature has datetime data type.\n",
        "\n",
        "- 2 features have floating-point (decimal) data type.\n",
        "\n",
        "- 9 features have integer (whole number) data type (4 stored as `int32` and 5 as `int64`).\n",
        "\n",
        "- 5 features have object (string) data type.\n",
        "\n",
        "The dataset has no null values (all Non-Null Counts match the number of rows).\n",
        "\n",
        "The memory requirement for this dataset is approximately 28.2 MB.\n"
      ],
      "metadata": {
        "id": "lX4fVEDv0YpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2.3.3. Checking for duplicates values :"
      ],
      "metadata": {
        "id": "cGXot-1V2UuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the number of rows in `data` is the same as that of `transactionData`, we can be sure that no duplicates were created. This is because we created `data` by setting `how = 'left'` (in other words, a left join) which means take all the rows in `transactionData` and find rows with matching values in shared columns and then joining the details in these rows to the `LYLTY_CARD_NBR` or the first mentioned table."
      ],
      "metadata": {
        "id": "Ff0q4Ea65Z0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2.3.4. Checking for missing values :"
      ],
      "metadata": {
        "id": "9-ZZZYQO7Ou9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also check if some customers were not matched on by checking for nulls."
      ],
      "metadata": {
        "id": "vsMKP0yQ5lvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_customer = data[data['LIFESTAGE'].isna()]   # we can also use isnull() instead of isna() .\n",
        "print(\"The missing customer data:\")\n",
        "display(missing_customer)"
      ],
      "metadata": {
        "id": "UEYS5_QD5gBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, there are no nulls! So all our customers in the transaction data has been accounted for in the customer dataset.\n"
      ],
      "metadata": {
        "id": "skL-orjL5ppg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this, the data is now ready for analysis. Data cleaning and transformations are always done to almost all real-world datasets. This includes handling for missing values, casting data to appropriate data types, standardizing or normalizing values, feature engineering, and date time and string types transformations, among others. Since this particular dataset only requires some transformations and not much cleaning, we can now move one.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dsm2p-4F25-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3. Exploratory Data Analysis including visualization :"
      ],
      "metadata": {
        "id": "AVNcSTvH8wjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.1. Store-wide Sales :"
      ],
      "metadata": {
        "id": "mbEz5cMcWWEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate sales by store\n",
        "store_sales = data.groupby(\"STORE_NBR\")[\"TOT_SALES\"].sum().reset_index()\n",
        "\n",
        "# Sort by sales (descending)\n",
        "store_sales = store_sales.sort_values(by=\"TOT_SALES\", ascending=False)\n",
        "\n",
        "# Pick top 10 high-selling stores\n",
        "top_stores = store_sales.head(10).sort_values(by=\"TOT_SALES\", ascending=False)\n",
        "\n",
        "# Show the table of top stores\n",
        "print(\"Top 10 High-Selling Stores:\")\n",
        "display(top_stores)\n",
        "\n",
        "# Plot only top stores\n",
        "plt.figure()\n",
        "sns.barplot(x=\"STORE_NBR\", y=\"TOT_SALES\", data=top_stores, palette=\"viridis\")\n",
        "plt.title(\"Top 10 High-Selling Stores by Total Sales\")\n",
        "plt.xlabel(\"Store Number\")\n",
        "plt.ylabel(\"Total Sales\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JQcFH4OuGYLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**ðŸ“Š Explanation : Top 10 High-Selling Stores by Total Sales**\n",
        "\n",
        "As depicted in the bar graph above, we can conclude that certain stores significantly outperform others in terms of their total sales.  \n",
        "To study this, the dataset was grouped using the `groupby()` function in Python on the `STORE_NBR` column, and the corresponding total sales for each store were aggregated using `.sum()`.  \n",
        "The results were then sorted in descending order, and the **top 10 stores by total sales** were visualized using the **Seaborn library**.  \n",
        "\n",
        "- **Store 226** has the highest overall sales, followed closely by Stores **88** and **165**.  \n",
        "- Stores such as **40, 58, and 237** also show consistently strong sales performance, placing them in the higher tier.  \n",
        "- In contrast, while Stores **4, 26, 199, and 203** make it to the top 10 list, their cumulative sales are relatively lower than the leading stores.  \n",
        "\n",
        "âœ… **Key Insight :**  \n",
        "\n",
        "The results show that **a small number of stores are responsible for most of the revenue in the retail business**.  \n",
        "\n",
        "ðŸ“Œ **Interpretation :**  \n",
        "\n",
        "This may be due to factors like the **size of the store**, **its location**, and the **range of products** it offers.  \n",
        "By understanding these factors, **businesses can make better choices about resource allocation, marketing activities, and store expansion decisions**.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LCrU_VT1TyQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter dataset for that best selling store\n",
        "top_store_nbr = top_stores.iloc[0][\"STORE_NBR\"]\n",
        "top_store_sales = data[data[\"STORE_NBR\"] == top_store_nbr].groupby(\"BRANDS\")[\"TOT_SALES\"].sum().reset_index()\n",
        "\n",
        "# Sort top products in that store\n",
        "top_store_sales = top_store_sales.sort_values(\"TOT_SALES\", ascending=False)\n",
        "\n",
        "# Show the table of top stores\n",
        "print(\"Seles of Brands in top selling Store:\")\n",
        "display(top_store_sales)\n",
        "\n",
        "# Plot sales of top products in highest selling store\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=\"TOT_SALES\", y=\"BRANDS\", data=top_store_sales, palette=\"viridis\")\n",
        "plt.title(f\"Sales of different brands in Store {int(top_store_nbr)} (Highest Selling Store)\")\n",
        "plt.xlabel(\"Total Sales\")\n",
        "plt.ylabel(\"Brands\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vXqfsRXLD4nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**ðŸ“Š Explanation : Brand Sales within Store 226 (Highest-Selling Store)**\n",
        "\n",
        "As depicted in the bar graph above, the sales contribution of different brands within **Store 226** (**the highest-selling store**) can be observed.  \n",
        "The dataset was filtered specifically for this store, and total sales were aggregated across each brand using the `groupby()` function in Python.  \n",
        "The results were then sorted in descending order and visualized using the **Seaborn library**.  \n",
        "\n",
        "- **Kettle** is the dominant brand in Store 226, with sales figures far exceeding those of other brands, crossing the **4,000** mark.  \n",
        "- **Pringles** and **Doritos** follow as the next highest-selling brands, although their sales are roughly half of Kettleâ€™s total, highlighting a significant performance gap.  \n",
        "- Other brands such as **Smiths, Twisties, and Thins** also show strong contributions but remain in the mid-tier range.  \n",
        "- Meanwhile, brands like **Cheezels, Grain, and Infzns** account for comparatively lower sales, making up the lower tier in this storeâ€™s brand performance.  \n",
        "\n",
        "âœ… **Key Insight :**\n",
        "\n",
        "This analysis suggests that **a few key brands drive the majority of sales within the top-performing store**.  \n",
        "\n",
        "ðŸ“Œ **Interpretation :**  \n",
        "\n",
        "This implies that **stocking strategies, promotional offers, and marketing campaigns** for these dominant brands could have a significant impact on overall store revenue.  \n",
        "Conversely, understanding why lower-performing brands contribute less may help in **optimizing shelf space, rethinking product placement, or tailoring promotions** to boost their sales.  \n"
      ],
      "metadata": {
        "id": "KU0PFXiaXKF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2. Product-level Sales Trends :"
      ],
      "metadata": {
        "id": "zjszFUYbYsJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_sales = data.groupby([\"BRANDS\", \"PACK_SIZE\"])[\"TOT_SALES\"].sum().reset_index()\n",
        "\n",
        "top_products = product_sales.sort_values(\"TOT_SALES\", ascending=False).head(10)\n",
        "\n",
        "# Show the table of top products(pack sizes)\n",
        "print(\"Top 10 High-Selling Products :\")\n",
        "display(top_products)\n",
        "\n",
        "plt.figure()\n",
        "sns.barplot(x=\"BRANDS\", y=\"TOT_SALES\", data=top_products, hue=\"PACK_SIZE\")\n",
        "plt.legend(title = \"Pack Size\")\n",
        "plt.title(\"Top 10 Product-level Sales Trends\")\n",
        "plt.xlabel(\"Brands\")\n",
        "plt.ylabel(\"Total Sales\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XjMgQPFdGYIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**ðŸ“Š Explanation : Top 10 Product-level Sales Trends**\n",
        "\n",
        "As shown in the bar graph above, the **top 10 product-level sales trends** are illustrated by aggregating sales across both **brand** and **pack size**.  \n",
        "The dataset was grouped using the `groupby()` function on the `BRANDS` and `PACK_SIZE` columns, and the total sales (`TOT_SALES`) were computed.  \n",
        "The results were then sorted in descending order, and the highest-selling brandâ€“pack combinations were visualized using the **Seaborn library**.  \n",
        "\n",
        "- **Kettle (175g pack)** is the leading product, achieving close to **200,000** in total sales.  \n",
        "- This is followed by **Pringles (134g pack)** and **Kettle (150g pack)**, both of which show strong performance with sales figures exceeding **170,000**.  \n",
        "- **Doritos (170g pack)** ranks next with sales above **100,000**, while other products such as **Thins (175g), Tostitos (175g), Cobs (110g), Infuzions (110g), Smiths (110g), and Twisties (270g)** make up the remaining positions in the top 10.  \n",
        "\n",
        "âœ… **Key Insight :**  \n",
        "\n",
        "- **Mid-sized packs (110gâ€“175g)** dominate sales compared to smaller or very large pack sizes, suggesting that customers prefer moderate pack sizes that balance price and quantity.  \n",
        "- The dominance of brands such as **Kettle, Pringles, and Doritos** highlights strong **brand loyalty** and **consumer preference** in this retail dataset.  \n",
        "\n",
        "ðŸ“Œ **Interpretation :**  \n",
        "\n",
        "This analysis suggests that both **brand strength** and **optimal pack size** are critical drivers of product-level sales.  \n",
        "Understanding these patterns can guide **inventory planning, product assortment strategies, and promotional campaigns**, with a focus on high-performing product-pack combinations, while also reevaluating the role of lower-selling sizes.  \n"
      ],
      "metadata": {
        "id": "Q1SlQHY_atC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.3. Brand-level Performance :"
      ],
      "metadata": {
        "id": "IE0fC_lBcFb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brand_sales = data.groupby(\"BRANDS\")[\"TOT_SALES\"].sum().sort_values(ascending=False).reset_index()\n",
        "top_brands = brand_sales.sort_values(\"TOT_SALES\", ascending=False).head(10)\n",
        "\n",
        "# Show the table of top products(pack sizes)\n",
        "print(\"Top 10 High-Selling brands :\")\n",
        "display(top_brands)\n",
        "\n",
        "plt.figure()\n",
        "sns.barplot(x=\"TOT_SALES\", y=\"BRANDS\", data=brand_sales, palette=\"viridis\")\n",
        "plt.title(\"Brands by Sales\")\n",
        "plt.xlabel(\"Total Sales\")\n",
        "plt.ylabel(\"Brands\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9ogOYHwlGYEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**ðŸ“Š Explanation : Brands by Sales**\n",
        "\n",
        "As illustrated in the bar chart above, the sales contribution of different **brands** has been computed by aggregating the `TOT_SALES` column across all transactions.  \n",
        "The dataset was grouped by the `BRANDS` field using the `groupby()` function in Python, followed by summation of total sales.  \n",
        "The results were then sorted in descending order and visualized using the **Seaborn library**.  \n",
        "\n",
        "- **Kettle** is the leading brand, generating sales close to **400,000**, which is significantly higher than all other brands.  \n",
        "- **Smiths, Doritos, and Pringles** follow as other strong performers, with sales ranging between **150,000 and 200,000**.  \n",
        "- Mid-tier brands such as **Thins, Red Rock Deli, Twisties, and Tostitos** contribute steadily but at much lower levels compared to the top four.  \n",
        "- Brands such as **Infuzions, CCs, Cheetos, and Sunbites** represent the lower end of the sales spectrum with comparatively minimal contributions.  \n",
        "\n",
        "ðŸ“Œ **Interpretation :**\n",
        "\n",
        "This analysis suggests that a handful of brands dominate the overall sales performance, with **Kettle alone contributing disproportionately** to total revenue.  \n",
        "Such dominance reflects both **brand loyalty and customer preference**, and it highlights the importance of prioritizing **inventory management, marketing strategies, and promotional campaigns** for the top-selling brands.  \n",
        "Lower-performing brands may require targeted actions such as **discounts, better shelf placement, or repositioning** to improve their sales.  \n"
      ],
      "metadata": {
        "id": "R-_xUNiNc6L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.4. Demographics :"
      ],
      "metadata": {
        "id": "-DghMUWcisth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_sales = data.groupby([\"LIFESTAGE\", \"PREMIUM_CUSTOMER\"])[\"TOT_SALES\"].sum().reset_index()\n",
        "\n",
        "print(f\"The total sales by LIFESTAGE and PREMIUM_CUSTOMER : \")\n",
        "display(demo_sales.sort_values(\"TOT_SALES\", ascending=False))\n",
        "\n",
        "plt.figure()\n",
        "sns.barplot(x=\"TOT_SALES\", y=\"LIFESTAGE\", hue=\"PREMIUM_CUSTOMER\", data=demo_sales.sort_values(\"TOT_SALES\", ascending=False))\n",
        "plt.title(\"Sales by Customer Demographics\")\n",
        "plt.legend(title = \"Premium Customer\")\n",
        "plt.ylabel(\"Customerâ€™s life stage group\")\n",
        "plt.xlabel(\"Total Sales\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hysSwz47PfqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ðŸ“Š Explanation : Sales by Customer Demographics**\n",
        "\n",
        "The chart shows **total sales segmented by customer life stage group and premium status** (Budget, Mainstream, Premium).\n",
        "\n",
        "- **Older Families** generate the highest total sales, particularly within the **Budget segment**, indicating they are a dominant customer group for this product category.  \n",
        "- **Young Singles/Couples** and **Retirees** are also major contributors, with **Mainstream customers** leading in both groups.  \n",
        "- **Young Families** show strong sales, mostly driven by **Budget customers**, followed closely by **Mainstream customers**.  \n",
        "- **Older Singles/Couples** also contribute significantly across all three premium categories, showing a fairly **balanced distribution**.  \n",
        "- **Midage Singles/Couples** sales are moderate, with **Mainstream being the largest contributor**.  \n",
        "- **New Families** contribute the least overall, suggesting they are a relatively smaller consumer base in this dataset.  \n",
        "\n",
        "\n",
        "âœ… **Key Insight :**\n",
        "\n",
        "- **Life stage matters**: Sales are concentrated in **Older Families, Young Singles/Couples, Retirees, and Young Families** â€” these groups should be the focus of marketing and promotions.  \n",
        "- **Premium segmentation :**  \n",
        "  - **Budget customers dominate** in *Older Families* and *Young Families*.  \n",
        "  - **Mainstream customers dominate** in *Young Singles/Couples* and *Retirees*.  \n",
        "  - **Premium customers** are strongest in *Older Singles/Couples*, showing they may be more **brand/quality-oriented** in that stage.  \n",
        "- **Smaller groups** (e.g., *New Families, Midage Singles/Couples*) represent niche markets but may be growth opportunities with targeted campaigns.  \n",
        "\n",
        "\n",
        "ðŸ“Œ **Interpretation :**\n",
        "\n",
        "This analysis suggests that **family life stage** and **spending profile** (budget vs. mainstream vs. premium) are critical factors in sales performance.  \n",
        "\n",
        "- To maximize revenue, businesses should prioritize **Older Families, Young Singles/Couples, and Retirees**, tailoring promotions to their premium status (*budget vs. mainstream*).  \n",
        "- **Premium positioning** works well with **Older Singles/Couples**, while **budget-friendly offers** resonate with **family groups**.  \n",
        "- For **underrepresented groups** (e.g., *New Families*), specialized campaigns could unlock **new sales potential**.  \n",
        "\n"
      ],
      "metadata": {
        "id": "8g3ecn8pf8O0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.5. Basket/Transaction Analysis :"
      ],
      "metadata": {
        "id": "aG-uPmX5kEV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "basket_size = data.groupby(\"TXN_ID\")[\"PROD_QTY\"].sum()\n",
        "basket_value = data.groupby(\"TXN_ID\")[\"TOT_SALES\"].sum()\n",
        "\n",
        "print(f\"The basket size and basket value : \")\n",
        "display(basket_size.value_counts())\n",
        "\n",
        "plt.figure()\n",
        "sns.histplot(basket_size, bins=30, kde=True)\n",
        "plt.title(\"Distribution of Basket Size (Items per Transaction)\")\n",
        "plt.xlabel(\"Number of Items\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nThe basket value : \")\n",
        "display(basket_value.value_counts())\n",
        "\n",
        "plt.figure()\n",
        "sns.histplot(basket_value, bins=30, kde=True)\n",
        "plt.title(\"Distribution of Basket Value (Sales per Transaction)\")\n",
        "plt.xlabel(\"Total Sales\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KotGbnX4Pfm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**ðŸ“Š Interpretation : Basket Size & Basket Value Distributions**\n",
        "\n",
        "##### **1. Distribution of Basket Size (Items per Transaction)**\n",
        "- The **basket size distribution** is highly concentrated around **2 items per transaction**, which represents the most common shopping behavior.  \n",
        "- A small number of transactions consist of only **1 item**, and very few transactions exceed **3 or more items**.  \n",
        "- This indicates that the majority of customers purchase **snack products in small quantities**, treating them as impulse or supplementary buys rather than bulk purchases.  \n",
        "\n",
        "âœ… **Key Insight :**\n",
        "\n",
        "Most transactions are light baskets (1â€“2 items), so promotions encouraging multi-buy offers (e.g., *â€œBuy 2, Get 1 Freeâ€*) could potentially increase basket size.  \n",
        "\n",
        "\n",
        "##### **2. Distribution of Basket Value (Sales per Transaction)**\n",
        "- The **basket value distribution** is right-skewed, with most transactions falling between **5 and 10** dollr in sales value.  \n",
        "- A peak is observed around (7â€“9), showing this is the typical spend per transaction.  \n",
        "- Higher-value transactions (>15) are relatively rare, indicating that bulk or premium purchases are exceptions rather than the norm.  \n",
        "\n",
        "âœ… **Key Insight :**\n",
        "\n",
        "Customers usually spend within a small range, reinforcing the idea of snack purchases being **low-ticket items**. Retailers could introduce **bundle pricing** or **upsell strategies** to push average spend slightly higher.  \n",
        "\n",
        "\n",
        "ðŸ“Œ **Interpretation :**\n",
        "\n",
        "- The dataset reflects a **convenience/snack-driven purchasing pattern** with small baskets and modest spending.  \n",
        "- Business opportunities lie in **nudging customers towards slightly larger basket sizes and higher-value transactions** through promotions, product bundling, or targeted offers.  \n"
      ],
      "metadata": {
        "id": "Mx1VRDl6j_xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.6. Product Quantity vs Customer Segment :"
      ],
      "metadata": {
        "id": "GzI6L19WkBSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qty_vs_segment = data.groupby([\"LIFESTAGE\", \"PREMIUM_CUSTOMER\"])[\"PROD_QTY\"].mean().reset_index()\n",
        "\n",
        "print(f\"The average product quantity by customer segment : \")\n",
        "display(qty_vs_segment.sort_values(\"PROD_QTY\", ascending=False))\n",
        "\n",
        "plt.figure()\n",
        "sns.barplot(x=\"PROD_QTY\", y=\"LIFESTAGE\", hue=\"PREMIUM_CUSTOMER\", data=qty_vs_segment)\n",
        "plt.title(\"Average Product Quantity by Customer Segment\")\n",
        "plt.legend(title = \"Premium Customer\")\n",
        "plt.ylabel(\"Customerâ€™s life stage group\")\n",
        "plt.xlabel(\"Average Product Quantity\")\n",
        "plt.xlim(0,2.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XSa4YZ0SPfjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**ðŸ“Š Interpretation : Average Product Quantity by Customer Segment**\n",
        "\n",
        "**Observations :**\n",
        "- Across **all life stages and premium customer groups**, the **average product quantity per transaction is close to 2 items**.  \n",
        "- **Variation is minimal** across different segments â€” indicating a consistent purchasing pattern regardless of life stage or premium classification.  \n",
        "- Slight differences are observed:  \n",
        "  - **Young Families** and **Older Families** show marginally higher averages, suggesting family groups may purchase slightly larger quantities.  \n",
        "  - **Young Singles/Couples** tend to have a slightly lower average basket size compared to other groups.  \n",
        "  - Differences between **Budget, Mainstream, and Premium** customers are very small, showing that spending tier has little effect on average item quantity.  \n",
        "\n",
        "âœ… **Key Insight :**\n",
        "\n",
        "- **Customer life stage** has a modest impact: Families (both young and older) lean toward buying marginally more products per transaction than singles/couples.  \n",
        "- **Premium segmentation** (Budget vs. Mainstream vs. Premium) is not a strong driver of product quantity â€” customer type does not significantly change basket size.  \n",
        "- The overall **stability of basket quantity (~2 items)** aligns with earlier findings that transactions are generally small, reinforcing the snack/convenience nature of purchases.  \n",
        "\n",
        "ðŸ“Œ **Interpretation :**\n",
        "\n",
        "This analysis suggests that, while different demographic groups (life stages) influence **who buys more**, the differences are relatively small.  \n",
        "\n",
        "- **Promotional campaigns** may need to focus less on increasing *quantity per basket* (since itâ€™s already stable) and more on **increasing basket value** through upselling or premium product positioning.  \n",
        "- For **family segments**, bundle or multi-pack deals could resonate better, while **singles/couples** may be more responsive to value-driven promotions.  \n"
      ],
      "metadata": {
        "id": "RuAgAtMmp0jW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.7. Monthly/Quarterly Trends :"
      ],
      "metadata": {
        "id": "ECtP6TncmPvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temporal_sales = data.groupby([\"YEAR\", \"MONTH\"])[\"TOT_SALES\"].sum().reset_index()\n",
        "temporal_sales[\"Period\"] = temporal_sales[\"YEAR\"].astype(str) + \"-\" + temporal_sales[\"MONTH\"].astype(str)\n",
        "\n",
        "print(f\"The monthly sales trends : \")\n",
        "display(temporal_sales[['Period','TOT_SALES']])\n",
        "\n",
        "plt.figure()\n",
        "sns.lineplot(x=\"Period\", y=\"TOT_SALES\", data=temporal_sales, marker=\"o\")\n",
        "plt.title(\"Monthly Sales Trends\")\n",
        "plt.xlabel(\"Time Period\")\n",
        "plt.ylabel(\"Total Sales\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5iZsj74bPfdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ðŸ“Š Interpretation : Monthly Sales Trends**\n",
        "\n",
        "**Observations :**\n",
        "- Overall sales remain **fairly stable**, fluctuating around **148K â€“ 156K** across the months.  \n",
        "- **Highest peak :** December 2018 approximately (156K) and March 2019 (~156K), suggesting strong seasonal or promotional sales.  \n",
        "- **Lowest point :** February 2019 (~141K), showing a sharp dip compared to surrounding months.  \n",
        "- Sales show **repeated surges around year-end and early Quatre2**, hinting at periodic demand spikes.  \n",
        "\n",
        "âœ… **Key Insight :**\n",
        "\n",
        "- **Seasonality effect :**  \n",
        "  - December peak likely tied to holiday shopping.  \n",
        "  - March rebound could reflect post-holiday restocking or seasonal promotions.  \n",
        "- **Sales dip in February :**  \n",
        "  - Consistent with shorter month length and possibly reduced consumer spending after holiday season.  \n",
        "- **Steady baseline :** Outside of peaks and dips, sales hover in a narrow range (~148Kâ€“153K), showing stable demand.  \n",
        "\n",
        "ðŸ“Œ **Interpretation :**\n",
        "\n",
        "The data indicates that while the business enjoys stable monthly sales, there are clear **seasonal spikes and dips**.  \n",
        "\n",
        "- **December** is a major sales driver â†’ leverage with targeted promotions, gift bundles, and holiday campaigns.  \n",
        "- **February slump** â†’ opportunity to counteract with loyalty rewards or off-season discounts.  \n",
        "- Maintaining strong performance in **March** suggests potential for quarterly promotions timed around this rebound.  \n"
      ],
      "metadata": {
        "id": "Yl5A-ZkPrMi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.8. Week-over-Week Sales :"
      ],
      "metadata": {
        "id": "57TjBvK2mr6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_sales = data.groupby([\"YEAR\", \"WEEK\"])[\"TOT_SALES\"].sum().reset_index()\n",
        "\n",
        "print(f\"The weekly sales trends : \")\n",
        "display(weekly_sales.sort_values(\"TOT_SALES\", ascending=False))\n",
        "\n",
        "plt.figure()\n",
        "sns.lineplot(x=\"WEEK\", y=\"TOT_SALES\", hue=\"YEAR\", data=weekly_sales, marker=\"o\", palette=\"tab10\")\n",
        "plt.title(\"Week-over-Week Sales Trends\")\n",
        "plt.legend(title=\"Year\")\n",
        "plt.xlabel(\"Week Number\")\n",
        "plt.ylabel(\"Total Sales\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VboeUXOjGYBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ðŸ“Š Interpretation : Week-over-Week Sales Trends**\n",
        "\n",
        "**Observations :**\n",
        "- **High sales consistency :** Weeks 1â€“4 show very stable sales levels, staying around **205Kâ€“210K** in both years (2018 and 2019).  \n",
        "- **Sharp drop in Week 5 :** Sales plummet drastically â€” ~80K in 2018 and ~65K in 2019, marking the lowest point in the period.  \n",
        "- **Year-over-year comparison :**\n",
        "  - Sales in 2018 and 2019 track very closely across all weeks.  \n",
        "  - The Week 5 decline is more severe in 2019 than 2018.  \n",
        "\n",
        "âœ… **Key Insight :**\n",
        "\n",
        "- **Strong start of the month :** Consistent high sales in the first four weeks suggest stable consumer demand.  \n",
        "- **End-of-month drop :** The steep fall in Week 5 indicates a structural trend â€” likely due to shorter purchase cycles, paycheck timing, or fewer shopping days in those weeks.  \n",
        "- **Year-over-year similarity :** Since both years follow the same weekly pattern, the trend seems to be systemic rather than random.  \n",
        "\n",
        "ðŸ“Œ **Interpretation :**\n",
        "\n",
        "This analysis highlights that sales are **front-loaded within each month**, with the bulk occurring in the first four weeks.  \n",
        "\n",
        "- Businesses can **capitalize early in the month** with promotions when sales are naturally strong.  \n",
        "- **Week 5 is a weak sales period** â€” targeted discounts or marketing campaigns could help offset the slowdown.  \n",
        "- Since the trend repeats across years, planning inventory and marketing around this **monthly sales cycle** will optimize performance.  \n"
      ],
      "metadata": {
        "id": "Cq9OMMm0rtO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.9. Temporal Segment Analysis :"
      ],
      "metadata": {
        "id": "la-VvQ0km3Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temporal_segment = data.groupby([\"YEAR\", \"MONTH\", \"LIFESTAGE\"])[\"TOT_SALES\"].sum().reset_index()\n",
        "temporal_segment[\"Period\"] = temporal_segment[\"YEAR\"].astype(str) + \"-\" + temporal_segment[\"MONTH\"].astype(str)\n",
        "\n",
        "print(f\"The sales trends by customer segment : \")\n",
        "display(temporal_segment[[\"Period\", \"LIFESTAGE\", \"TOT_SALES\"]].sort_values(\"TOT_SALES\", ascending=False))\n",
        "\n",
        "plt.figure()\n",
        "sns.lineplot(x=\"Period\", y=\"TOT_SALES\", hue=\"LIFESTAGE\", data=temporal_segment, marker=\"o\")\n",
        "plt.title(\"Sales Trends by Customer Segment Over Time\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel(\"Total Sales\")\n",
        "plt.ylim(0,52000)\n",
        "plt.legend(title=\"Customer Segment\", title_fontsize=11, fontsize=9, loc='upper right', markerscale=1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_LHBhdyBGX9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ðŸ“Š Interpretation : Sales Trends by Customer Segment Over Time**\n",
        "\n",
        "**Observations :**\n",
        "- **Older Singles/Couples** consistently lead sales, averaging around **30Kâ€“33K** per month, showing their dominance as the largest customer group.  \n",
        "- **Retirees** and **Older Families** follow closely, maintaining sales in the **27Kâ€“30K** range, indicating strong and stable purchasing power.  \n",
        "- **Young Families** and **Young Singles/Couples** contribute moderate sales (~20Kâ€“25K), with slight month-to-month fluctuations.  \n",
        "- **Midage Singles/Couples** show lower sales (~13Kâ€“15K), remaining steady but relatively smaller contributors.  \n",
        "- **New Families** are consistently the **smallest group**, with sales below **5K**, highlighting their limited role in total sales.\n",
        "\n",
        "âœ… **Key Insight :**\n",
        "\n",
        "- **Stability across segments :** Most segments follow a stable pattern over time, with only minor seasonal variations.  \n",
        "- **Core customer base :** Older demographics (Singles/Couples, Families, Retirees) drive the majority of sales, showing brand loyalty and steady demand.  \n",
        "- **Growth opportunities :** Younger and midage groups underperform compared to older segments, suggesting potential for targeted promotions to capture more share.  \n",
        "- **New Families as niche :** Their consistently low sales indicate either limited demand or misalignment with the product category â€” marketing campaigns may need to be re-evaluated.\n",
        "\n",
        "ðŸ“Œ **Interpretation :**\n",
        "\n",
        "This trend analysis reveals that **sales are heavily concentrated in older life-stage groups**, making them the key drivers of revenue.  \n",
        "For sustained growth, the business should **continue catering to older customer segments** while **designing targeted strategies for younger and family-oriented groups** to expand their sales contribution over time.\n"
      ],
      "metadata": {
        "id": "on5yr6-eswsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.10. Correlation Matrix :"
      ],
      "metadata": {
        "id": "bjBB1IqknCLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = data[[\"PROD_QTY\", \"SELLING_PRICE\", \"TOT_SALES\"]].corr()\n",
        "\n",
        "plt.figure()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", center=0, cbar_kws={'label': 'Correlation'})\n",
        "plt.title(\"Correlation Matrix : Price, Quantity, Sales\")\n",
        "plt.xlabel(\"Variables\")\n",
        "plt.ylabel(\"Variables\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jZ7OTwqIGXua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ðŸ“Š Correlation Analysis : Price, Quantity, and Sales**  \n",
        "\n",
        "**Observations :**  \n",
        "- **TOT_SALES vs SELLING_PRICE â†’ Strong positive correlation (0.84)**  \n",
        "  - Revenue is heavily influenced by product price.  \n",
        "\n",
        "- **TOT_SALES vs PROD_QTY â†’ Moderate positive correlation (0.54)**  \n",
        "  - Higher sales volume contributes to revenue but less strongly than price.  \n",
        "\n",
        "- **SELLING_PRICE vs PROD_QTY â†’ Very weak correlation (0.026)**  \n",
        "  - Price and quantity sold are nearly independent â†’ price changes donâ€™t strongly affect sales volume.  \n",
        "\n",
        "âœ… **Key Insight :**  \n",
        "\n",
        "- **Pricing is the main revenue driver** â†’ Optimizing price has the greatest effect on total sales.  \n",
        "- **Quantity has secondary impact** â†’ Increasing units sold helps, but less than price adjustments.  \n",
        "- **Low priceâ€“quantity link** â†’ Suggests demand may be relatively **price inelastic**, giving flexibility for pricing strategies.  \n",
        "\n",
        "ðŸ“Œ **Interpretation :**\n",
        "\n",
        "The analysis highlights that **selling price is the most critical factor** for driving revenue, while sales quantity plays a supporting role.  \n",
        "\n",
        "- Use **price optimization** (e.g., discounts, premium pricing, bundling) to maximize revenue.  \n",
        "- Support with **quantity-based promotions** (e.g., â€œbuy more, save moreâ€) to boost units sold.  \n",
        "- Since demand volume shows little dependence on price, the business can experiment with pricing without risking large drops in sales quantity.  \n"
      ],
      "metadata": {
        "id": "HcbBPKlNzu10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.11. Price Sensitivity by Demographics :"
      ],
      "metadata": {
        "id": "3oMJ0P6_nfIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "price_segment = data.groupby([\"LIFESTAGE\", \"PREMIUM_CUSTOMER\"])[\"SELLING_PRICE\"].mean().reset_index()\n",
        "\n",
        "print(f\"The average price paid by customer segment : \")\n",
        "display(price_segment.sort_values(\"SELLING_PRICE\", ascending=False))\n",
        "\n",
        "plt.figure()\n",
        "sns.barplot(x=\"SELLING_PRICE\", y=\"LIFESTAGE\", hue=\"PREMIUM_CUSTOMER\", data=price_segment)\n",
        "plt.title(\"Average Price Paid by Customer Segment\")\n",
        "plt.legend(title = \"Premium Customer\")\n",
        "plt.ylabel(\"Customerâ€™s life stage group\")\n",
        "plt.xlabel(\"Average Selling Price\")\n",
        "plt.xlim(0,5.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "V5QkZaZzGXrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ðŸ“Š Interpretation : Average Price Paid by Customer Segment**  \n",
        "\n",
        "**Observations :**  \n",
        "- Across all **lifestages**, the **average selling price** is fairly consistent, ranging around **3.5 â€“ 4.2**.  \n",
        "- **Mainstream customers** tend to pay slightly more than **Budget** and **Premium** segments in several groups (e.g., *Young Singles/Couples*, *Midage Singles/Couples*).  \n",
        "- Differences across **LIFESTAGE groups** are minor, showing no extreme price variation by age or family status.  \n",
        "- **Retirees, Older Singles/Couples, and Young Families** consistently fall in the **3.8 â€“ 4.0** price range across all segments.  \n",
        "\n",
        "âœ… **Key Insight :**\n",
        "\n",
        "- **Price stability :** Pricing strategy appears uniform across customer groups, suggesting little price discrimination between lifestages.  \n",
        "- **Mainstream customers paying more :** Indicates possible willingness among the **middle-market** segment to accept higher prices.  \n",
        "- **Premium segment not always highest :** Surprising findingâ€”Premium customers are not consistently paying the most, which could suggest effective discounting or loyalty benefits targeted at them.  \n",
        "\n",
        "ðŸ“Œ **Interpretation :**\n",
        "\n",
        "The visualization suggests that **average price sensitivity is low across demographic (lifestage) groups**, and instead, **purchase behavior by segment type (Budget, Mainstream, Premium)** has more influence.  \n",
        "\n",
        "- **Opportunity with mainstream customers** â†’ They already accept slightly higher prices, making them ideal for upselling or bundling strategies.  \n",
        "- **Premium customers** â†’ May be receiving discounts or targeting deals; business could revisit if this aligns with profitability goals.  \n",
        "- **Budget customers** â†’ Their spending is close to Premium, meaning pricing tiers are not sharply differentiatedâ€”potential risk of **cannibalization** between segments.  \n"
      ],
      "metadata": {
        "id": "EUvHglY51PVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.12. Pack Size Preference :"
      ],
      "metadata": {
        "id": "mGsf4YGanlpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pack_pref = data.groupby(\"PACK_SIZE\")[\"TOT_SALES\"].sum().sort_values(ascending=False).reset_index()\n",
        "\n",
        "print(f\"The Pack size preferences : \")\n",
        "display(pack_pref.sort_values(\"TOT_SALES\", ascending=False))\n",
        "\n",
        "plt.figure()\n",
        "sns.barplot(x=\"PACK_SIZE\", y=\"TOT_SALES\", data=pack_pref, palette=\"viridis\")\n",
        "plt.title(\"Pack Size Preferences\")\n",
        "plt.xlabel(\"Pack Size\")\n",
        "plt.ylabel(\"Total Sales\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "429HaYNzGXj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ðŸ“Š Interpretation : Top 10 Pack Size Preferences **\n",
        "\n",
        "**Observations :**  \n",
        "- **175 pack size** dominates sales by a huge margin (~480K), far ahead of all other pack sizes.  \n",
        "- The **next highest sellers** are **150 (~285K)** and **134 (~175K)** pack sizes, but they are still significantly lower than the 175 size.  \n",
        "- Mid-range pack sizes like **110, 170, 330, and 165** contribute moderately (~140Kâ€“180K).  \n",
        "- Larger pack sizes (e.g., **380, 270, 210**) show noticeably lower sales compared to smaller sizes.  \n",
        "- Very small and niche sizes (e.g., **70, 90, 125, 160, 180, 200, 220**) contribute minimally to total sales.  \n",
        "\n",
        "âœ… **Key Insight :**\n",
        "\n",
        "- **Strong consumer preference for smaller pack sizes**: 175, 150, and 134 dominate, suggesting convenience or affordability drives purchase decisions.  \n",
        "- **Mid-size packs (110â€“330 range)** still perform reasonably well, appealing to a broad customer base.  \n",
        "- **Large packs have weaker demand**: Possibly due to higher upfront cost or storage constraints.  \n",
        "- **Skewed sales distribution**: A few pack sizes account for the majority of revenue, while many others contribute marginally.  \n",
        "\n",
        "ðŸ“Œ **Interpretation :**\n",
        "\n",
        "The analysis shows that **sales are highly concentrated in a few pack sizes**, especially the **175 pack**, which could be the companyâ€™s flagship offering.  \n",
        "\n",
        "- **Focus marketing and availability** on top-performing pack sizes (175, 150, 134) to sustain strong sales.  \n",
        "- **Evaluate underperforming pack sizes** â†’ consider reducing SKUs or repositioning them (e.g., bundling large packs with discounts).  \n",
        "- **Mid-range packs** provide a balanced opportunity â†’ maintain supply for varied customer preferences.  \n",
        "- Insights suggest customers value **affordability, convenience, and portion control** when choosing pack sizes.  \n"
      ],
      "metadata": {
        "id": "L7OFb1SH1zHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.13. Store & Brand Interaction :"
      ],
      "metadata": {
        "id": "cqWwF9XZo0n2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregating total sales by store and brand\n",
        "store_brand = data.groupby([\"STORE_NBR\", \"BRANDS\"])[\"TOT_SALES\"].sum().reset_index()\n",
        "\n",
        "# Finding top 10 selling brands overall\n",
        "top_brands = store_brand.groupby(\"BRANDS\")[\"TOT_SALES\"].sum().nlargest(10).index\n",
        "\n",
        "# Finding top 10 selling stores overall\n",
        "top_stores = store_brand.groupby(\"STORE_NBR\")[\"TOT_SALES\"].sum().nlargest(10).index\n",
        "\n",
        "# Filtering dataset for top 10 stores and brands\n",
        "filtered_data = store_brand[store_brand[\"BRANDS\"].isin(top_brands) & store_brand[\"STORE_NBR\"].isin(top_stores)]\n",
        "\n",
        "# Creating pivot table for heatmap\n",
        "pivot_brand = filtered_data.pivot_table(index=\"STORE_NBR\", columns=\"BRANDS\", values=\"TOT_SALES\").fillna(0)\n",
        "\n",
        "# Plot as heatmap\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(pivot_brand, cmap=\"YlGnBu\", annot=True, fmt=\".0f\", cbar_kws={'label': 'Sales'})\n",
        "plt.title(\"Top 10 Stores vs Top 10 Brands (Sales Heatmap)\")\n",
        "plt.ylabel(\"Store Number\")\n",
        "plt.xlabel(\"Brands\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ybmBcitcGXdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ðŸ“Š Interpretation : Top 10 Stores vs Top 10 Brands**  \n",
        "\n",
        "**Observations :**  \n",
        "- **Kettle** is the strongest-performing brand across all top stores, with sales consistently above **3,600â€“4,300**, making it the clear leader.  \n",
        "- **Doritos** is the second-best performer, with sales ranging **~1,700â€“2,060**, showing strong popularity.  \n",
        "- **Pringles** and **Smiths** sit in the mid-range, with sales between **1,500â€“2,000** and **900â€“1,300**, respectively.  \n",
        "- Smaller brands like **Twisties, Tostitos, Thins, Cobs, Infuzions** generate lower but steady sales (**600â€“1,000** per store).  \n",
        "- Store **226** leads in total sales across multiple brands, especially **Kettle (4,301)** and **Pringles (2,013)**.  \n",
        "- Stores **88, 165, and 226** consistently perform well across most brands, indicating strong customer traffic or brand affinity.  \n",
        "\n",
        "âœ… **Key Insight :**\n",
        "\n",
        "- **Brand dominance** â†’ Kettle is the primary sales driver, with Doritos as the next key contributor.  \n",
        "- **Cross-store consistency** â†’ Kettle and Doritos are strong across all stores, showing broad consumer appeal.  \n",
        "- **Store-specific strengths** â†’ Stores like **226 and 88** outperform others, making them ideal for promotions or launches.  \n",
        "- **Smaller brandsâ€™ role** â†’ Though weaker individually, niche brands (Cobs, Infuzions, Thins, Twisties) add variety and maintain steady sales.  \n",
        "\n",
        "ðŸ“Œ **Interpretation :**\n",
        "\n",
        "The heatmap reveals that **a few brands dominate overall sales (Kettle and Doritos)**, while secondary brands provide product diversity.  \n",
        "\n",
        "- **Focus on top brands :** Ensure stock availability and targeted promotions for Kettle and Doritos.  \n",
        "- **Leverage high-performing stores :** Use stores like **226 and 88** as pilots for campaigns or new product launches.  \n",
        "- **Revisit weaker brands :** Consider repositioning, bundling, or promotions for consistently underperforming brands.  \n",
        "- **Balance assortment :** Maintain variety to serve niche customer preferences while prioritizing leading brands.  \n"
      ],
      "metadata": {
        "id": "MKxoWFvv2XBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "##4. Conclusion :\n"
      ],
      "metadata": {
        "id": "Kx9tOscaCZBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Conclusions from the Analysis :**  \n",
        "\n",
        "1. **Store-level Insights :**  \n",
        "   - A handful of stores (e.g., **Store 226, 88, and 165**) generate the majority of sales.  \n",
        "   - This indicates that **store size, location, and customer base** play a significant role in driving revenue.  \n",
        "   - These top-performing stores can be leveraged for **piloting new campaigns, product launches, or targeted promotions**.  \n",
        "\n",
        "2. **Brand-level Insights :**  \n",
        "   - **Kettle** is the dominant brand, consistently leading both overall and store-specific sales.  \n",
        "   - Other strong performers include **Doritos, Pringles, and Smiths**, while smaller brands contribute marginally.  \n",
        "   - This shows that **brand loyalty** and **consumer preference** are concentrated in a few key brands.  \n",
        "\n",
        "3. **Product & Pack Size Preferences :**  \n",
        "   - **175g packs** are the most preferred, far ahead of other sizes.  \n",
        "   - Mid-sized packs (150gâ€“175g) dominate the sales landscape, while larger packs perform poorly.  \n",
        "   - Customers prefer **convenient, moderately priced pack sizes**, suggesting impulse-driven or household-oriented consumption.  \n",
        "\n",
        "4. **Customer Demographics :**  \n",
        "   - **Older Families, Retirees, and Young Singles/Couples** are the largest contributors to sales.  \n",
        "   - **Budget and Mainstream customers** dominate across most life stages, while Premium customers contribute selectively.  \n",
        "   - Demographics strongly influence **what customers buy and how much they spend**, but average product quantity per basket remains stable across groups.  \n",
        "\n",
        "5. **Basket Analysis :**  \n",
        "   - The majority of baskets contain only **1â€“2 items**, with an average spend of **$5â€“$10 per transaction**.  \n",
        "   - This highlights the **snack/convenience nature** of purchases rather than bulk buying.  \n",
        "   - Upselling strategies (e.g., **â€œBuy 2, Get 1 Freeâ€**) could effectively increase basket size.  \n",
        "\n",
        "6. **Temporal Sales Trends :**  \n",
        "   - Sales remain stable overall, but **seasonality effects** are evident:  \n",
        "     - Peaks in **December (holiday season)** and **March (quarterly rebound)**.  \n",
        "     - A slump in **February**, likely due to the short month and post-holiday slowdown.  \n",
        "   - Week-over-week analysis shows a strong **front-loading of sales in Weeks 1â€“4**, with sharp declines in **Week 5**.  \n",
        "   - These cyclical patterns can help businesses **time promotions and manage inventory more effectively**.  \n",
        "\n",
        "7. **Correlation Analysis :**  \n",
        "   - **Selling price has the strongest influence on total sales (correlation = 0.84)**, compared to product quantity.  \n",
        "   - Price and quantity are largely independent, suggesting demand is relatively **price inelastic**.  \n",
        "   - This provides businesses with flexibility in **pricing strategies** without drastically impacting volume.  \n",
        "\n",
        "8. **Storeâ€“Brand Interaction :**  \n",
        "   - **Kettle and Doritos** consistently dominate across top stores.  \n",
        "   - Some stores (e.g., **226 and 88**) outperform others across multiple brands, making them key strategic outlets.  \n",
        "   - Secondary brands provide product diversity but contribute less to overall revenue.  \n",
        "\n",
        "###**Final Interpretation :**  \n",
        "\n",
        "- The retail dataset shows that **revenue is highly concentrated** in a few stores, brands, and pack sizes.  \n",
        "- Customer purchases are **small and frequent**, reflecting the snack/convenience nature of products.  \n",
        "- **Demographics and seasonality** play an important role in shaping sales patterns, while **price optimization** emerges as the most critical factor for maximizing revenue.  \n",
        "\n",
        "  \n",
        "\n",
        "###**Overall Conclusion :**\n",
        "\n",
        "The analysis highlights that **success in this retail business is driven by a few dominant stores, brands, and pack sizes, combined with stable but seasonal demand patterns**.  \n",
        "By **optimizing pricing, focusing on high-value customer segments, and tailoring inventory and marketing strategies**, businesses can significantly enhance sales performance and profitability.  \n"
      ],
      "metadata": {
        "id": "YOB35msFEG_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##5. Recommendations :"
      ],
      "metadata": {
        "id": "Y1RAlyZZKPQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Recommendations for Future Work :**\n",
        "\n",
        "1. **Targeted Promotions & Campaigns :**  \n",
        "   - Leverage high-performing stores (e.g., Store 226, 88) for **new product launches and pilot campaigns**.  \n",
        "   - Use **multi-buy promotions** to encourage larger basket sizes.  \n",
        "   - Seasonal campaigns in **December** and **March** can maximize demand, while **February dips** can be mitigated with discounts or loyalty offers.  \n",
        "\n",
        "2. **Inventory & Assortment Planning :**  \n",
        "   - Prioritize stocking of **top-performing pack sizes (175g, 150g, 134g)**.  \n",
        "   - Re-evaluate underperforming large pack sizes; consider repositioning, bundling, or SKU rationalization.  \n",
        "   - Ensure consistent availability of top brands like **Kettle, Doritos, and Pringles** across all stores.  \n",
        "\n",
        "3. **Customer Segmentation Strategy :**  \n",
        "   - Focus on **Older Families, Retirees, and Young Singles/Couples** as primary target groups.  \n",
        "   - Develop differentiated promotions for **Budget vs. Premium customers**, since both contribute significantly but exhibit different buying behaviors.  \n",
        "   - Explore **untapped potential in New Families and Midage groups** with specialized marketing.  \n",
        "\n",
        "4. **Pricing Strategy :**  \n",
        "   - Since demand is **price inelastic**, experiment with **premium pricing** for high-demand products.  \n",
        "   - Introduce **bundle discounts** to increase quantity purchased per transaction.  \n",
        "   - Monitor price sensitivity across different demographics to refine targeted offers.  \n",
        "\n",
        "5. **Future Analytical Enhancements :**  \n",
        "   - Perform **time series forecasting** (ARIMA, Prophet, etc.) for predicting sales trends and planning inventory.  \n",
        "   - Apply **market basket analysis (association rules)** to discover product affinities and cross-sell opportunities.  \n",
        "   - Conduct **customer segmentation (clustering models)** to refine targeting beyond life stage and premium classification.  \n",
        "   - Use **predictive models** (e.g., regression, XGBoost) to evaluate the impact of price, promotions, and demographics on sales."
      ],
      "metadata": {
        "id": "JoItoODJJhGR"
      }
    }
  ]
}